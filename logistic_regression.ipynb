{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import sklearn.preprocessing, sklearn.metrics, sklearn.utils, sklearn.multiclass, sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('data/fma_metadata/features.csv', index_col=0, header=[0, 1, 2])\n",
    "tracks = pd.read_csv('data/fma_metadata/tracks.csv', index_col=0, header=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CENS = 'chroma_cens'\n",
    "CQT = 'chroma_cqt'\n",
    "STFT = 'chroma_stft'\n",
    "MFCC = 'mfcc'\n",
    "RMSE = 'rmse'\n",
    "BW = 'spectral_bandwidth'\n",
    "CENT = 'spectral_centroid'\n",
    "CONT = 'spectral_contrast'\n",
    "ROLLOFF = 'spectral_rolloff'\n",
    "TON = 'tonnetz'\n",
    "ZCR = 'zcr'\n",
    "all_features = [CENS,CQT,STFT,MFCC,RMSE,BW,CENT,CONT,ROLLOFF,TON,ZCR]\n",
    "\n",
    "class LR(object):\n",
    "    def __init__(self, tracks, features):\n",
    "        self.tracks = tracks\n",
    "        self.features = features\n",
    "        self.small = tracks['set', 'subset'] <= 'small'\n",
    "        self.training = tracks['set', 'split'] == 'training'\n",
    "        self.validation = tracks['set', 'split'] == 'validation'\n",
    "        self.testing = tracks['set', 'split'] == 'test'\n",
    "    \n",
    "    \n",
    "    def datasplit(self, feature_array):\n",
    "        # takes an array of features [MFCC, CONT]\n",
    "        X_train_temp = self.features.loc[self.small & (self.training | self.validation), feature_array]\n",
    "        X_test_temp = self.features.loc[self.small & self.testing, feature_array]\n",
    "        y_train_temp = self.tracks.loc[self.small & (self.training | self.validation), ('track', 'genre_top')]\n",
    "        y_test_temp = self.tracks.loc[self.small & self.testing, ('track', 'genre_top')]\n",
    "        y_train = y_train_temp.dropna()\n",
    "        y_test = y_test_temp.dropna()\n",
    "        X_train = X_train_temp.drop(y_train_temp.drop(y_train.index).index)\n",
    "        X_test = X_test_temp.drop(y_test_temp.drop(y_test.index).index)\n",
    "        EXPERIMENTAL = self.tracks['track', 'genre_top'] == \"Experimental\"\n",
    "        X_train = X_train.drop(X_train.loc[EXPERIMENTAL].index)\n",
    "        y_train = y_train.drop(y_train.loc[EXPERIMENTAL].index)\n",
    "        X_test = X_test.drop(X_test.loc[EXPERIMENTAL].index)\n",
    "        y_test = y_test.drop(y_test.loc[EXPERIMENTAL].index)\n",
    "        return skl.utils.shuffle(X_train, y_train, random_state=42), X_test, y_test\n",
    "\n",
    "    # given parameters achieves the highest score\n",
    "    def train(self, feature_array=all_features, solver='liblinear', penalty='l1', multi_class='ovr', C=0.072):\n",
    "        (X_train, y_train), X_test, y_test = self.datasplit(feature_array)\n",
    "        scaler = skl.preprocessing.StandardScaler(copy=False)\n",
    "        scaler.fit_transform(X_train)\n",
    "        scaler.transform(X_test)\n",
    "        \n",
    "        # LR through OneVsOne scheme        \n",
    "        base_lr = skl.linear_model.LogisticRegression(solver=solver,penalty=penalty,multi_class=multi_class,C=C, random_state=0)\n",
    "        self.classifier = skl.multiclass.OneVsOneClassifier(base_lr)\n",
    "        print(self.classifier)\n",
    "        self.classifier.fit(X_train, y_train)        \n",
    "        print(\"Logistic Regression OneVsOne(solver =\", solver, \")\")\n",
    "        \n",
    "        print(\"Training Report:\", self.classifier.score(X_train,y_train))\n",
    "        print(skl.metrics.classification_report(y_train, self.classifier.predict(X_train)))\n",
    "        print()\n",
    "        print(\"Test Report: Accuracy = \", self.classifier.score(X_test,y_test))\n",
    "        print(skl.metrics.classification_report(y_test, self.classifier.predict(X_test)))\n",
    "        \n",
    "    def cross_validation(self,feature_array=all_features):\n",
    "        (X_train, y_train), X_test, y_test = self.datasplit(feature_array)\n",
    "        scaler = skl.preprocessing.StandardScaler(copy=False)\n",
    "        scaler.fit_transform(X_train)\n",
    "        scaler.transform(X_test)\n",
    "        \n",
    "        scores = ['precision', 'recall']\n",
    "\n",
    "        for score in scores:\n",
    "            print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "            print()\n",
    "            \n",
    "            tuned_parameters = {'C':10.0 ** -np.arange(1, 7), 'penalty':['l1'],\n",
    "                                'solver':['liblinear','saga'], 'multi_class':['ovr'], 'random_state':[0]}\n",
    "            clf = skl.model_selection.GridSearchCV(skl.linear_model.LogisticRegression(), tuned_parameters, cv=5, scoring='%s_macro' % score)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            print(\"Best parameters set found on development set:\")\n",
    "            print()\n",
    "            print(clf.best_params_)\n",
    "            print()\n",
    "            print(\"Grid scores on development set:\")\n",
    "            print()\n",
    "            means = clf.cv_results_['mean_test_score']\n",
    "            stds = clf.cv_results_['std_test_score']\n",
    "            for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "                print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                      % (mean, std * 2, params))\n",
    "            print()\n",
    "\n",
    "            print(\"Detailed classification report:\")\n",
    "            print()\n",
    "            print(\"The model is trained on the full development set.\")\n",
    "            print(\"The scores are computed on the full evaluation set.\")\n",
    "            print()\n",
    "            y_true, y_pred = y_test, clf.predict(X_test)\n",
    "            print(classification_report(y_true, y_pred))\n",
    "            print()          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsOneClassifier(estimator=LogisticRegression(C=0.072, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          n_jobs=None, penalty='l1', random_state=0, solver='liblinear',\n",
      "          tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=None)\n",
      "Logistic Regression OneVsOne(solver = liblinear )\n",
      "Training Report: 0.7176859127661998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\defne\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.00      0.00      0.00        97\n",
      "          Classical       0.86      0.82      0.84      1143\n",
      "            Country       0.00      0.00      0.00       176\n",
      "     Easy Listening       0.00      0.00      0.00        18\n",
      "         Electronic       0.69      0.81      0.74      8533\n",
      "               Folk       0.65      0.63      0.64      2504\n",
      "            Hip-Hop       0.73      0.66      0.69      3229\n",
      "       Instrumental       0.54      0.35      0.43      1770\n",
      "      International       0.68      0.46      0.54      1261\n",
      "               Jazz       0.73      0.33      0.46       524\n",
      "Old-Time / Historic       0.95      0.93      0.94       499\n",
      "                Pop       0.49      0.13      0.20      2128\n",
      "               Rock       0.75      0.89      0.82     12718\n",
      "           Soul-RnB       0.00      0.00      0.00       132\n",
      "             Spoken       0.59      0.55      0.57       392\n",
      "\n",
      "          micro avg       0.72      0.72      0.72     35124\n",
      "          macro avg       0.51      0.44      0.46     35124\n",
      "       weighted avg       0.69      0.72      0.69     35124\n",
      "\n",
      "\n",
      "Test Report: Accuracy =  0.6619244697361614\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              Blues       0.00      0.00      0.00        13\n",
      "          Classical       0.80      0.70      0.75        87\n",
      "            Country       0.00      0.00      0.00        18\n",
      "     Easy Listening       0.00      0.00      0.00         6\n",
      "         Electronic       0.65      0.81      0.72       839\n",
      "               Folk       0.37      0.29      0.32       299\n",
      "            Hip-Hop       0.77      0.72      0.75       323\n",
      "       Instrumental       0.52      0.17      0.25       309\n",
      "      International       0.60      0.32      0.42       128\n",
      "               Jazz       0.76      0.47      0.58        47\n",
      "Old-Time / Historic       0.77      0.98      0.86        55\n",
      "                Pop       0.14      0.03      0.06       204\n",
      "               Rock       0.70      0.89      0.78      1464\n",
      "           Soul-RnB       0.00      0.00      0.00        43\n",
      "             Spoken       0.51      0.61      0.56        31\n",
      "\n",
      "          micro avg       0.66      0.66      0.66      3866\n",
      "          macro avg       0.44      0.40      0.40      3866\n",
      "       weighted avg       0.61      0.66      0.62      3866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\defne\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lr = LR(tracks,features)\n",
    "lr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LR(tracks,features)\n",
    "lr.cross_validation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
